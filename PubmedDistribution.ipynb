{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e55ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig\n",
    "from model import InstructGLM\n",
    "from bertviz import model_view, head_view\n",
    "import torch\n",
    "import random\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(0)\n",
    "# model\n",
    "model_name = '/home/zuographgroup/zhr/model/vicuna-7b-v1.5'\n",
    "device = torch.device('cuda:1')\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "special={'additional_special_tokens': ['<Node {}>'.format(i) for i in range(1, 21)]}   # Add a new special token as place holder\n",
    "tokenizer.add_special_tokens(special)\n",
    "config = LlamaConfig.from_pretrained(model_name)\n",
    "model = InstructGLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    config=config,\n",
    "#     use_cache=True, \n",
    "#     low_cpu_mem_usage=True,\n",
    "    device_map={\"\":device}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from model import GraphEncoder\n",
    "import numpy as np\n",
    "from circuitsvis.attention import attention_patterns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(args=[])\n",
    "args.gnn_type = 'GraphSAGE'\n",
    "args.gnn_output = 4096\n",
    "args.num_token = 5\n",
    "args.gnn_input = 128\n",
    "args.gt_layers = 2\n",
    "args.att_d_model = 2048\n",
    "args.graph_pooling = 'sum'\n",
    "args.edge_dim = None\n",
    "args.dropout = 0.5\n",
    "args.graph_unsup = False\n",
    "args.prefix = 'graphsage_1000tp_5token_512_neg0_arxiv_linear_1_3400'\n",
    "args.dataset = 'arxiv'\n",
    "args.mask_token_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e58867",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48cde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model\n",
    "first_model_path = './saved_model/first_model/{}_fm_{}_epoch{}_{}.pth'\n",
    "llama_embeds = model.get_input_embeddings().weight.data\n",
    "node_token=torch.zeros(110, llama_embeds.shape[1]).to(device=device, dtype=llama_embeds.dtype)\n",
    "llama_embeds=torch.cat([llama_embeds, node_token],dim=0)\n",
    "first_model = GraphEncoder(args, llama_embed=llama_embeds).to(device, dtype=torch.bfloat16)\n",
    "first_model.load_state_dict(torch.load(first_model_path.format(args.prefix, args.dataset, 0, 'end')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff29ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "add_tokens = \" \".join([f\"<Node {i}>\" for i in range (1, 1 + 5)])\n",
    "df = pd.read_json('./instruction/pubmed/mark_ans.json')\n",
    "# print(df[df.mark == 2])\n",
    "idx = 59\n",
    "instruction = df.iloc[idx]\n",
    "llm_out, glm_out, real_out = instruction['llm_out'], instruction['glm_out'], instruction['output']\n",
    "print(llm_out, glm_out, real_out)\n",
    "raw_prompt = instruction['prompt']\n",
    "prompt = (raw_prompt.split('Abstract: ')[0] + 'Title: ' + raw_prompt.split('Title: ')[1])\n",
    "llm_prompt = 'Given a paper with the following information:\\n' + 'Title: ' + raw_prompt.split('Title: ')[1]\n",
    "# llm_prompt = 'Given a paper with the following information:\\n' + 'Abstract: ' + raw_prompt.split('Abstract: ')[1]\n",
    "print(prompt)\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Batch\n",
    "import tqdm\n",
    "\n",
    "add_tokens = \" \".join([f\"<Node {i}>\" for i in range (1, 1 + 5)])\n",
    "df = pd.read_json('./instruction/pubmed/mark_ans_1.json')\n",
    "filter_df = df[df.mark == 2]\n",
    "print(len(filter_df))\n",
    "filter_df = filter_df.sample(n=200)\n",
    "layer_num = 31\n",
    "y_all = []\n",
    "\n",
    "for idx, row in tqdm.tqdm(filter_df.iterrows(), total=len(filter_df)):\n",
    "    llm_out, glm_out, real_out = row['llm_out'], row['glm_out'], row['output']\n",
    "\n",
    "    raw_prompt = row['prompt']\n",
    "    prompt = (raw_prompt.split('Abstract: ')[0] + 'Title: ' + raw_prompt.split('Title: ')[1])\n",
    "    glm_prompt_ans = f\"\"\"USER: {prompt.replace(\": <Node 1>\", add_tokens)} ASSISTANT: {glm_out}\"\"\"\n",
    "    final_prompt = glm_prompt_ans + '</s>'\n",
    "    final_ans = glm_out\n",
    "    # final_prompt = glm_prompt\n",
    "    input_ids = tokenizer(\n",
    "        final_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "    targets = input_ids.clone()\n",
    "    target_len = len(tokenizer.tokenize(final_ans))\n",
    "    targets[:, :-target_len-1] = -100\n",
    "\n",
    "    graph = Data()\n",
    "    graph.edge_index = torch.LongTensor(row['edge_index'])\n",
    "    graph.edge_attr = None\n",
    "    node_list = row['node_set']\n",
    "    graph.x = torch.tensor(row['x'], dtype=torch.bfloat16)\n",
    "    graph.lp = False\n",
    "\n",
    "    is_node = (input_ids >= 32000)\n",
    "    extra_num = is_node.sum()\n",
    "\n",
    "    graph = Batch.from_data_list([graph])\n",
    "\n",
    "    embeds = first_model(\n",
    "        input_ids=input_ids.to(device),\n",
    "        is_node=is_node.to(device),\n",
    "        graph=graph.to(device),\n",
    "        use_llm=False\n",
    "    )\n",
    "    output = model(inputs_embeds=embeds, labels=targets, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    attn = output['attentions'][layer_num][0].mean(dim=0)\n",
    "    # attn = output['attentions'][layer_num][0][0]\n",
    "    tokens = tokenizer.encode(final_prompt)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    select_id = []\n",
    "    info_id = 0\n",
    "    query_id = 0\n",
    "    answer_id = 0\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in ['<Node 1>', '<Node 2>', '<Node 3>', '<Node 4>', '<Node 5>']:\n",
    "            select_id.append(i)\n",
    "        if tokens[i] == '‚ñÅQuestion':\n",
    "            query_id = i\n",
    "        elif tokens[i] == 'Title' and tokens[i-1] == '<0x0A>':\n",
    "            info_id = i\n",
    "        elif tokens[i] == 'IST' and tokens[i-1] == 'SS':\n",
    "            answer_id = i-1\n",
    "    select_attn = attn[-target_len-1, :]\n",
    "    assert answer_id > query_id\n",
    "    assert query_id > info_id\n",
    "\n",
    "    y = select_attn.detach().cpu().to(torch.float32).numpy()\n",
    "    y_mean = [y[select_id].max(), y[info_id:answer_id].max()]\n",
    "    y_all.append(y_mean)\n",
    "\n",
    "y_all = np.array(y_all)\n",
    "print(y_all.shape)\n",
    "\n",
    "plt.bar(np.arange(2), y_all.mean(axis=0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Batch\n",
    "import tqdm\n",
    "\n",
    "add_tokens = \" \".join([f\"<Node {i}>\" for i in range (1, 1 + 5)])\n",
    "df = pd.read_json('./instruction/pubmed/mark_ans_1.json')\n",
    "filter_df1 = df[df.mark == 1]\n",
    "filter_df2 = df[df.mark == 2]\n",
    "\n",
    "filter_df1 = filter_df1.sample(n=100)\n",
    "filter_df2 = filter_df2.sample(n=100)\n",
    "layer_num = 31\n",
    "\n",
    "diff_ls = []\n",
    "for head in range(32):\n",
    "    y_all = [[], []]\n",
    "    for row1, row2 in tqdm.tqdm(zip(filter_df1.iterrows(), filter_df2.iterrows()), total=len(filter_df)):\n",
    "        ls = [row1[1], row2[1]]\n",
    "        for row_id in range(2):\n",
    "            row = ls[row_id]\n",
    "            llm_out, glm_out, real_out = row['llm_out'], row['glm_out'], row['output']\n",
    "\n",
    "            raw_prompt = row['prompt']\n",
    "            prompt = (raw_prompt.split('Abstract: ')[0] + 'Title: ' + raw_prompt.split('Title: ')[1])\n",
    "            glm_prompt_ans = f\"\"\"USER: {prompt.replace(\": <Node 1>\", add_tokens)} ASSISTANT: {glm_out}\"\"\"\n",
    "            final_prompt = glm_prompt_ans + '</s>'\n",
    "            final_ans = glm_out\n",
    "            # final_prompt = glm_prompt\n",
    "            input_ids = tokenizer(\n",
    "                final_prompt,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "            attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "            targets = input_ids.clone()\n",
    "            target_len = len(tokenizer.tokenize(final_ans))\n",
    "            targets[:, :-target_len-1] = -100\n",
    "            \n",
    "            graph = Data()\n",
    "            graph.edge_index = torch.LongTensor(row['edge_index'])\n",
    "            graph.edge_attr = None\n",
    "            node_list = row['node_set']\n",
    "            graph.x = torch.tensor(row['x'], dtype=torch.bfloat16)\n",
    "            graph.lp = False\n",
    "\n",
    "            is_node = (input_ids >= 32000)\n",
    "            extra_num = is_node.sum()\n",
    "\n",
    "            graph = Batch.from_data_list([graph])\n",
    "\n",
    "            embeds = first_model(\n",
    "                input_ids=input_ids.to(device),\n",
    "                is_node=is_node.to(device),\n",
    "                graph=graph.to(device),\n",
    "                use_llm=False\n",
    "            )\n",
    "            output = model(inputs_embeds=embeds, labels=targets, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "            attn = output['attentions'][layer_num][0].mean(dim=0)\n",
    "            # attn = output['attentions'][layer_num][0][0]\n",
    "            tokens = tokenizer.encode(final_prompt)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            select_id = []\n",
    "            info_id = 0\n",
    "            query_id = 0\n",
    "            answer_id = 0\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] in ['<Node 1>', '<Node 2>', '<Node 3>', '<Node 4>', '<Node 5>']:\n",
    "                    select_id.append(i)\n",
    "                if tokens[i] == '‚ñÅQuestion':\n",
    "                    query_id = i\n",
    "                elif tokens[i] == 'Title' and tokens[i-1] == '<0x0A>':\n",
    "                    info_id = i\n",
    "                elif tokens[i] == 'IST' and tokens[i-1] == 'SS':\n",
    "                    answer_id = i-1\n",
    "            select_attn = attn[-target_len-1, :]\n",
    "            assert answer_id > query_id\n",
    "            assert query_id > info_id\n",
    "\n",
    "            y = select_attn.detach().cpu().to(torch.float32).numpy()\n",
    "            y_mean = [y[select_id].max(), y[info_id:answer_id].max()]\n",
    "            y_all[row_id].append(y_mean)\n",
    "    y_1 = np.array(y_all[0]).mean(axis=0)\n",
    "    y_2 = np.array(y_all[1]).mean(axis=0)\n",
    "    diff = np.absolute(y_1 - y_2).sum()\n",
    "    diff_ls.append(diff)\n",
    "print(diff_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_prompt = f\"\"\"USER: {llm_prompt} ASSISTANT:\"\"\"\n",
    "print(llama_prompt)\n",
    "glm_prompt = f\"\"\"USER: {prompt.replace(\": <Node 1>\", add_tokens)} ASSISTANT:\"\"\"\n",
    "print(glm_prompt)\n",
    "llama_prompt_ans = f\"\"\"USER: {llm_prompt} ASSISTANT: {llm_out}\"\"\"\n",
    "print(llama_prompt_ans)\n",
    "glm_prompt_ans = f\"\"\"USER: {prompt.replace(\": <Node 1>\", add_tokens)} ASSISTANT: {glm_out}\"\"\"\n",
    "print(glm_prompt_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = glm_prompt_ans + '</s>'\n",
    "final_ans = glm_out\n",
    "# final_prompt = glm_prompt\n",
    "input_ids = tokenizer(\n",
    "    final_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "print(input_ids)\n",
    "attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "targets = input_ids.clone()\n",
    "target_len = len(tokenizer.tokenize(final_ans))\n",
    "targets[:, :-target_len-1] = -100\n",
    "print(targets)\n",
    "\n",
    "graph = Data()\n",
    "graph.edge_index = torch.LongTensor(instruction['edge_index'])\n",
    "graph.edge_attr = None\n",
    "node_list = instruction['node_set']\n",
    "graph.x = torch.tensor(instruction['x'], dtype=torch.bfloat16)\n",
    "graph.lp = False\n",
    "\n",
    "is_node = (input_ids >= 32000)\n",
    "extra_num = is_node.sum()\n",
    "\n",
    "print(extra_num)\n",
    "\n",
    "graph = Batch.from_data_list([graph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec258b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "embeds = first_model(\n",
    "    input_ids=input_ids.to(device),\n",
    "    is_node=is_node.to(device),\n",
    "    graph=graph.to(device),\n",
    "    use_llm=False\n",
    ")\n",
    "results = model.g_step(in_embeds=embeds, attention_mask=attention_mask)\n",
    "# results = model.generate(input_ids.to(device), attention_mask=attention_mask)\n",
    "outputs = tokenizer.decode(results[0], skip_special_tokens=True).strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79846f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "embeds = first_model(\n",
    "    input_ids=input_ids.to(device),\n",
    "    is_node=is_node.to(device),\n",
    "    graph=graph.to(device),\n",
    "    use_llm=False\n",
    ")\n",
    "output = model(inputs_embeds=embeds, labels=targets, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)\n",
    "print(output['attentions'][0].shape)\n",
    "print(output['hidden_states'][-1].shape)\n",
    "print(output['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c2334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits of each layer\n",
    "logits_ls = []\n",
    "ans_token = tokenizer.encode(glm_out)[1]\n",
    "print(ans_token)\n",
    "print(tokenizer.convert_ids_to_tokens(ans_token))\n",
    "for i in range(1, 33):\n",
    "    layer_pred = model.lm_head(output['hidden_states'][i]).softmax(dim=-1)\n",
    "    logits_ls.append(layer_pred[:, -target_len-2, ans_token].detach().cpu().to(torch.float32).numpy())\n",
    "plt.plot(np.arange(32), logits_ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = output['attentions']\n",
    "tokens = tokenizer.encode(final_prompt)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "print(tokens)\n",
    "html = head_view(attn, tokens, include_layers=[0, 31], heads=[0], html_action='return')\n",
    "# html = model_view(attn, tokens, include_layers=[0, 31], html_action='return')\n",
    "with open(\"./head_view.html\", \"w\") as file:\n",
    "    file.write(html.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfe316",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 30\n",
    "layer_attn = output['attentions'][layer_num][0]\n",
    "tokens = tokenizer.encode(final_prompt)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "vis_tokens = [token[1:-1] if token in add_tokens else token for token in tokens]\n",
    "print(vis_tokens)\n",
    "vis = attention_patterns(tokens=vis_tokens, attention=layer_attn)\n",
    "vis_path = f\"./llm_vis_{idx}_{layer_num}.html\"\n",
    "with open(vis_path, \"w\") as f:\n",
    "    f.write(vis._repr_html_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6776c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 31\n",
    "attn = output['attentions'][layer_num][0].mean(dim=0)\n",
    "# attn = output['attentions'][layer_num][0][24]\n",
    "scale = attn.sum(dim=0)\n",
    "tokens = tokenizer.encode(final_prompt)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "select_id = []\n",
    "info_id = 0\n",
    "query_id = 0\n",
    "answer_id = 0\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i] in ['<Node 1>', '<Node 2>', '<Node 3>', '<Node 4>', '<Node 5>']:\n",
    "        select_id.append(i)\n",
    "    if tokens[i] == '‚ñÅQuestion':\n",
    "        query_id = i\n",
    "    elif tokens[i] == 'Title' and tokens[i-1] == '<0x0A>':\n",
    "        info_id = i\n",
    "    elif tokens[i] == 'IST' and tokens[i-1] == 'SS':\n",
    "        answer_id = i-1\n",
    "select_attn = attn[-target_len-1, :]\n",
    "print(select_attn.shape)\n",
    "print(select_id, info_id, query_id, answer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15303da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_np = scale.detach().cpu().to(torch.float32).numpy()\n",
    "y = select_attn.detach().cpu().to(torch.float32).numpy()\n",
    "x = np.arange(y.shape[0])\n",
    "y_max = np.array([y.max() for _ in range(y.shape[0])])\n",
    "\n",
    "y_mean = np.array([y[select_id].sum(), y[info_id:query_id].sum(), y[query_id:answer_id].sum()])\n",
    "# plt.plot(x, y, label=f'node {i}')\n",
    "plt.bar(np.arange(3), y_mean)\n",
    "\n",
    "# plt.fill_between(x[:info_id], y_max[:info_id], 0, facecolor = \"lightgreen\")\n",
    "# plt.fill_between(x[info_id:query_id], y_max[info_id:query_id], 0, facecolor = \"skyblue\")\n",
    "# plt.fill_between(x[query_id:], y_max[query_id:], 0, facecolor = \"lightgray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.1  # Êü±Áä∂ÂõæÁöÑÂÆΩÂ∫¶ÔºåÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÂíåÂÆ°ÁæéÊù•Êîπ\n",
    "offset = [- width*2, - width+0.01, 0.02, width+ 0.03, width*2 + 0.04]\n",
    "\n",
    "for i in range(5):\n",
    "    y_sq = np.array([y[:info_id, i].mean(), y[info_id:query_id, i].mean(), y[query_id:answer_id, i].mean(), y[answer_id:, i].mean()])\n",
    "    # y_sq = np.array([(y[:info_id, i]/scale_np[select_id[i]]).mean(), (y[info_id:query_id, i]/scale_np[select_id[i]]).mean(),\n",
    "    # (y[query_id:answer_id, i]/scale_np[select_id[i]]).mean(), (y[answer_id:, i]/scale_np[select_id[i]]).mean()])\n",
    "    plt.bar(np.arange(4)+offset[i], y_sq, label=f\"node {i}\", width=width)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attn_ls = []\n",
    "for i in range(32):\n",
    "    layer_attn = output['attentions'][i][0].mean(dim=0)\n",
    "    node_attn = layer_attn[:, select_id].mean(dim=0)\n",
    "    node_attn_ls.append(node_attn)\n",
    "node_attn_ls = torch.stack(node_attn_ls, dim=0).detach().cpu().to(torch.float32).numpy()\n",
    "for i in range(5):\n",
    "    plt.plot(np.arange(32), node_attn_ls[:, i], label=f\"node {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa6059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "glm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
